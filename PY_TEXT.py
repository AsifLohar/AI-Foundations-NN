# -*- coding: utf-8 -*-
"""LIL_Simple_Neural_Network_using_Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/gstripling00/introduction_to_neural_networks/blob/main/LIL_Simple_Neural_Network_using_Keras.ipynb

## Prediction Model for Sales Analysis



## Introduction
I created a prediction model for sales analysis. In this model, we need to feed the advertising budget of TV, radio, and newspapers to the model and the model will forecast the possible sales. For designing the model, the machine learning method I opted for is simple linear regression, and the programming was done in Jupyter notebook.

## Dataset
The advertising dataset captures the sales revenue generated with respect to advertisement costs across numerous platforms like radio, TV, and newspapers.

### Features:

#### Digital: advertising dollars spent on Internet.
#### TV: advertising dollars spent on TV.
#### Radio: advertising dollars spent on Radio.
#### Newspaper: advertising dollars spent on Newspaper.

### Target (Label):
#### Sales budget

### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Pandas, a data processing and CSV file I/O libraries
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns # Seaborn is a Python data visualization library based on matplotlib.
# %matplotlib inline
import sklearn

##Import the libraries
#import necessary libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping

# Keras specific
import keras
from keras.models import Sequential
from keras.layers import Dense

"""###  Load the Dataset

You will use the [USA housing prices](https://www.kaggle.com/kanths028/usa-housing) dataset found on Kaggle.  The data contains the following columns:

* 'Avg. Area Income': Avg. Income of residents of the city house is located in.
* 'Avg. Area House Age': Avg Age of Houses in same city
* 'Avg. Area Number of Rooms': Avg Number of Rooms for Houses in same city
* 'Avg. Area Number of Bedrooms': Avg Number of Bedrooms for Houses in same city
* 'Area Population': Population of city house is located in
* 'Price': Price that the house sold at
* 'Address': Address for the house
"""

# Next, you read the dataset into a Pandas dataframe.

url = 'https://github.com/gstripling00/introduction_to_neural_networks/blob/Datasets/Advertising_2023.csv?raw=true'
advertising_df= pd.read_csv(url,index_col=0)

advertising_df.head()

# Pandas info()Â function is used to get a concise summary of the dataframe.
advertising_df.info()

'''====== Data Exploration and Preprocessing ======'''

#shape of dataframe - 600 rows, five columns
advertising_df.shape

"""Let's check for any null values."""

# The isnull() method is used to check and manage NULL values in a data frame.
advertising_df.isnull().sum()

#check there are any NAN values
advertising_df.isnull().values.any()

'''=== show the statistics analysis of each attributes ==='''

#descriptive statistics
advertising_df.describe()

"""## Exploratory Data Analysis (EDA)

Let's create some simple plots to check out the data!  
"""

# The heatmap is a way of representing the data in a 2-dimensional form. The data values are represented as colors in the graph.
# The goal of the heatmap is to provide a colored visual summary of information.
sns.heatmap(advertising_df.corr())

# It is used basically for univariant set of observations and visualizes it through a histogram i.e. only one observation
# and hence you choose one particular column of the dataset.
sns.displot(advertising_df['sales'])

'''=== Show the linear relationship between features  and price. Thus, it provides that how the scattered
      they are and which features has more impact in prediction of house price. ==='''

# visiualize all variables  with price(MEDV)
from scipy import stats
#creates figure
plt.figure(figsize=(18, 18))

for i, col in enumerate(advertising_df.columns[0:13]): #iterates over all columns except for price column (last one)
    plt.subplot(5, 3, i+1) # each row three figure
    x = advertising_df[col] #x-axis
    y = advertising_df['sales'] #y-axis
    plt.plot(x, y, 'o')

    # Create regression line
    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1)) (np.unique(x)), color='red')
    plt.xlabel(col) # x-label
    plt.ylabel('sales') # y-label

"""## Training a Linear Regression Model

Regression is a supervised machine learning process.  It is similar to classification, but rather than predicting a label, you try to predict a continuous value.   Linear regression defines the relationship between a target variable (y) and a set of predictive features (x).  Simply stated, If you need to predict a number, then use regression.

Let's now begin to train your regression model! You will need to first split up your data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. You will toss out the Address column because it only has text info that the linear regression model can't use.

### X (features) and y (target) arrays

Next, let's define the features and label.  Briefly, feature is input; label is output. This applies to both classification and regression problems.
"""

X = advertising_df[['digital', 'TV', 'radio', 'newspaper']]
y = advertising_df['sales']

"""## Train - Test - Split

Now let's split the data into a training set and a testing set. You will train out model on the training set and then use the test set to evaluate the model.  Note that you are using 40% of the data for testing.  

#### What is Random State?
If an integer for random state is not specified in the code, then every time the code is executed, a new random value is generated and the train and test datasets will have different values each time.  However, if a fixed value is assigned -- like random_state = 0 or 1 or 101 or any other integer, then no matter how many times you execute your code the result would be the same, e.g. the same values will be in the train and test datasets.  Thus, the random state that you provide is used as a seed to the random number generator. This ensures that the random numbers are generated in the same order.
"""

# Import train_test_split function from sklearn.model_selection
from sklearn.model_selection import train_test_split

# Split up the data into a training set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)

## Build the model
model = Sequential()
model.add(Dense(4,activation='relu'))
model.add(Dense(3,activation='relu'))
model.add(Dense(1))

## Compile the Model
## Compilation - Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments

# for a mean squared error regression problem

model.compile(loss= "mean_squared_error" ,
              optimizer="adam",
              metrics=["mean_squared_error"])

#model.summary()

## SIMPLE:  Fit the Model
# train the model, iterating on the data in batches
# of 32 samples
model.fit(X_train, y_train, epochs=32)

history = model.fit(
    X_train, y_train,
    validation_data = (X_test, y_test),
    epochs = 150
)

# show the graph of model loss in trainig and validation

plt.figure(figsize=(15,8))
plt.xlabel('Epoch')
plt.ylabel('Mean Square Error')
plt.plot(history.epoch, history.history['loss'],
           label='Train Loss')
plt.plot(history.epoch, history.history['val_loss'],
           label = 'Val loss')
plt.title('Model loss')
plt.legend()

## TALK ABOUT CONVERGENCE

#Note that after 32 Epochs we could have "stopped early" rather than go ghrough 150 eochs since the
#numbers are so close MSE is around $163K.

## Build the model
model = Sequential()
model.add(Dense(32, input_dim=4, activation= "relu"))
model.add(Dense(32, activation= "relu"))
model.add(Dense(1, activation='linear', name='sales'))


## Compile the Model
model.compile(loss= "mse" ,
              optimizer="adam",
              metrics=["mse"])


## Fit the Model
history = model.fit(X_train, y_train,
          batch_size=32, epochs=100,
          validation_data = (X_test, y_test))

#Let's show loss as Mean Square Error (MSE)
loss, mse = model.evaluate(X_train)

print("Mean Squared Error", mse)

# show the graph of model loss in trainig and validation

# Use matplotlib to draw the model's loss curves for training and validation
def plot_curves(history, metrics):
    nrows = 1
    ncols = 2
    fig = plt.figure(figsize=(10, 5))

    for idx, key in enumerate(metrics):
        ax = fig.add_subplot(nrows, ncols, idx+1)
        plt.plot(history.history[key])
        plt.plot(history.history['val_{}'.format(key)])
        plt.title('model {}'.format(key))
        plt.ylabel(key)
        plt.xlabel('epoch')
        plt.legend(['train', 'validation'], loc='upper left');

plot_curves(history, ['loss', 'mse'])

'''=== Visualize the model evaluation skill ==='''


# visualize the prediction uisng diagonal line
y = test_predictions #y-axis
x = y_test #x-axis
fig, ax = plt.subplots(figsize=(10,6)) # create figure
ax.scatter(x,y) #scatter plots for x,y
ax.set(xlim=(0,55), ylim=(0, 55)) #set limit
ax.plot(ax.get_xlim(), ax.get_ylim(), color ='red') # draw 45 degree diagonal in figure
plt.xlabel('True Values')
plt.ylabel('Predicted values')
plt.title('Evaluation Result')
plt.show()

#check the model performace in test dataset
score = model.evaluate(X_test, y_test, verbose=1)
print('loss value: ', score[0])
print('Mean absolute error: ', score[1])



'''=== predict the house price ==='''

# predict house price using the test data
test_predictions = model.predict(X_test).flatten()
print(test_predictions)

# show the true value and predicted value in dataframe
true_predicted = pd.DataFrame(list(zip(y_test, test_predictions)),
                    columns=['True Value','Predicted Value'])
true_predicted.head(10)

# visualize the prediction uisng diagonal line
y = test_predictions #y-axis
x = y_test #x-axis
fig, ax = plt.subplots(figsize=(10,6)) # create figure
ax.scatter(x,y) #scatter plots for x,y
ax.set(xlim=(0,55), ylim=(0, 55)) #set limit
ax.plot(ax.get_xlim(), ax.get_ylim(), color ='red') # draw 45 degree diagonal in figure
plt.xlabel('True Values')
plt.ylabel('Predicted values')
plt.title('Evaluation Result')
plt.show()

## EVALUATE

"""NOTHING PAST HERE

## Creating and Training the Model

## Model Evaluation

Let's evaluate the model by checking out it's coefficients and how you can interpret them.

**Residual Histogram**

## Regression Evaluation Metrics


Here are three common evaluation metrics for regression problems:

**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:

$$\frac 1n\sum_{i=1}^n|y_i-\hat{y}_i|$$

**Mean Squared Error** (MSE) is the mean of the squared errors:

$$\frac 1n\sum_{i=1}^n(y_i-\hat{y}_i)^2$$

**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:

$$\sqrt{\frac 1n\sum_{i=1}^n(y_i-\hat{y}_i)^2}$$

Comparing these metrics:

- **MAE** is the easiest to understand, because it's the average error.
- **MSE** is more popular than MAE, because MSE "punishes" larger errors, which tends to be useful in the real world.
- **RMSE** is even more popular than MSE, because RMSE is interpretable in the "y" units.

All of these are **loss functions**, because you want to minimize them.
"""

# Importing metrics from sklearn
from sklearn import metrics

# Show the values of MAE, MSE, RMSE
print('MAE:', metrics.mean_absolute_error(y_test, test_predictions))
print('MSE:', metrics.mean_squared_error(y_test, test_predictions))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, test_predictions)))